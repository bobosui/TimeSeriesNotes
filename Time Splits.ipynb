{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.otexts.org/fpp/2/5\n",
    "\n",
    "For time series data, the procedure is similar but the training set consists only of observations that occurred prior to the observation that forms the test set. Thus, no future observations can be used in constructing the forecast. However, it is not possible to get a reliable forecast based on a very small training set, so the earliest observations are not considered as test sets. Suppose kk observations are required to produce a reliable forecast. Then the process works as follows.\n",
    "\n",
    "    Select the observation at time k+ik+i for the test set, and use the observations at times 1,2,…,k+i−11,2,…,k+i−1 to estimate the forecasting model. Compute the error on the forecast for time k+ik+i.\n",
    "    Repeat the above step for i=1,2,…,T−ki=1,2,…,T−k where TT is the total number of observations.\n",
    "    Compute the forecast accuracy measures based on the errors obtained.\n",
    "\n",
    "This procedure is sometimes known as a \"rolling forecasting origin\" because the \"origin\" (k+i−1k+i−1) at which the forecast is based rolls forward in time.\n",
    "\n",
    "With time series forecasting, one-step forecasts may not be as relevant as multi-step forecasts. In this case, the cross-validation procedure based on a rolling forecasting origin can be modified to allow multi-step errors to be used. Suppose we are interested in models that produce good hh-step-ahead forecasts.\n",
    "\n",
    "    Select the observation at time k+h+i−1k+h+i−1 for the test set, and use the observations at times 1,2,…,k+i−11,2,…,k+i−1 to estimate the forecasting model. Compute the hh-step error on the forecast for time k+h+i−1k+h+i−1.\n",
    "    Repeat the above step for i=1,2,…,T−k−h+1i=1,2,…,T−k−h+1 where TT is the total number of observations.\n",
    "    Compute the forecast accuracy measures based on the errors obtained.\n",
    "\n",
    "When h=1h=1, this gives the same procedure as outlined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://robjhyndman.com/hyndsight/crossvalidation/\n",
    "\n",
    "Cross-validation for time series\n",
    "\n",
    "When the data are not independent cross-validation becomes more difficult as leaving out an observation does not remove all the associated information due to the correlations with other observations. For time series forecasting, a cross-validation statistic is obtained as follows\n",
    "\n",
    "Fit the model to the data y_1, ..., y_t and let Y_t+1 denote the forecast of the next observation. Then compute the error\n",
    "for the forecast observation e_t+1.\n",
    "Repeat step 1 for t=m, ..., n-1, where m is THE MINIMUM NUMBER OF OBSERVATIONS NEEDED FOR FITTING THE MODEL (jak jeden się uczy szybciej, ale gorzej, to może mu wyjść lepszy wynik!)\n",
    "Compute the MSE from e_m+1, ..., e_n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.robjhyndman.com/papers/forecast-accuracy.pdf\n",
    "\n",
    "onclusions\n",
    "• Always calculate forecast accuracy measures using test data that was not used when\n",
    "computing the forecasts.\n",
    "• Use the MAE or RMSE if all your forecasts are on the same scale.\n",
    "• Use the MAPE if you need to compare forecast accuracy on several series with different scales, unless the data contain zeros or small values, or are not measuring a quantity.\n",
    "• Use the MASE if you need to compare forecast accuracy on several series with different scales, especially when the MAPE is inappropriate.\n",
    "• Use time series cross-validation where possible, rather than a simple training/test set split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Winsorizing or winsorization is the transformation of statistics by limiting extreme values in the statistical data to reduce the effect of possibly spurious outliers. It is named after the engineer-turned-biostatistician Charles P. Winsor (1895–1951). The effect is the same as clipping in signal processing.\n",
    "\n",
    "The distribution of many statistics can be heavily influenced by outliers. A typical strategy is to set all outliers to a specified percentile of the data; for example, a 90% winsorization would see all data below the 5th percentile set to the 5th percentile, and data above the 95th percentile set to the 95th percentile. Winsorized estimators are usually more robust to outliers than their more standard forms, although there are alternatives, such as trimming, that will achieve a similar effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Trimming - remove outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo - Leśniak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theil's U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kevinsheppard.com/images/0/0a/Kreiss_and_lahiri.pdf - bootstrap for time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://robjhyndman.com/papers/cv-wp.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.ssc.wisc.edu/~kwest/publications/2000/Forecast%20Evaluation.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
