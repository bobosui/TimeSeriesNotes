{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The problem of chaotic time series prediction is studied in various disciplines:\n",
    "- engineering,\n",
    "- medical,\n",
    "- econometric applications.\n",
    "\n",
    "Czy faktycznie tak jest? Czy szeregi czasowe w grancie Reliability są chaotyczne?\n",
    "http://www.fujitsu.com/global/about/resources/news/press-releases/2016/0216-01.html\n",
    "Now Fujitsu Laboratories has developed deep learning technology that uses advanced chaos theory and topology to automatically and accurately classify volatile time-series data. This can accurately handle even complex time-series data with severe oscillations.\n",
    "Czy jesteśmy w stanie jakkolwiek znaleźć/zgadnąć, co robi fujitsu?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fujitsu.jpg\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Chaotic time series are the output of a deterministic system with positive Liapunov exponent.\n",
    "\n",
    "Ścisła definicja nie jest aż tak istotna - ważne aby zapamiętać, że dowolnie mała zmiana warunków początkowych skutkuje dowolnie dużą zmianą po pewnym czasie."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Before 1980s:\n",
    "Linear parametric autoregressive - AR\n",
    "Moving-average - MA\n",
    "Autoregressive moving-average - ARMA\n",
    "\n",
    "Było u Igora, będzie u Igora S.?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The dependences are minimal in random time series and maximal in a complete deterministic process. But, random and deterministic are only margin of the large set of chaotic time series signals with weak dependences between components on short or long term. A special case is represented by the fractal time series characterized by auto similarity or non-periodic cycles."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It is shown that the recurrent NN (RNN) with a sufficiently large number of neurons is a realization of the nonlinear ARMA (NARMA) process."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Although NN had been shown to be universal approximators, it has found that NN had difficulty modeling seasonal patterns in time series. When a time series contains significant seasonality, the data need to be deseasonalized.\n",
    "\n",
    "Jak się robi \"desezonalizację\"?\n",
    "\n",
    "Za Wikipedią: \n",
    "Seasonality may be caused by various factors, such as weather, vacation, and holidays[2] and usually consists of periodic, repetitive, and generally regular and predictable patterns in the levels[3] of a time series. Seasonality can repeat on a weekly, monthly or quarterly basis, these periods of time are structured and occur in a length of time less than a year. Seasonal fluctuations in a time series can be contrasted with cyclical patterns. The latter occurs when the data exhibits rises and falls that are not of a fixed period. These fluctuations are usually due to economic conditions and are often related to the \"business cycle.\" The fixed period of time usually extends beyond a single year and the fluctuations are usually of at least two year[4] and do not repeat over fixed periods of time.\n",
    "\n",
    "a także:\n",
    "A seasonal pattern occurs when a time series is affected by seasonal factors such as the time of the year or the day of the week.\n",
    "A cycle occurs when the data exhibit rises and falls that are not of a fixed period. These fluctuations are usually due to economic conditions and are often related to the \"business cycle\".\n",
    "It is important to distinguish cyclic patterns and seasonal patterns. Seasonal patterns have a fixed and known length, while cyclic patterns have variable and unknown length. The average length of a cycle is usually longer than that of seasonality, and the magnitude of cyclic variation is usually more variable than that of seasonal variation."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The number of samples in time series. Researchers have found that increasing observation frequency does not always help to improve the accuracy of prediction.\n",
    "\n",
    "Potencjalnie ważna uwaga! Może jak np. mamy pomiary co sekundę, to trzeba je uśrednić godzinami?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To evaluate the prediction capability of proposed algorithms, the best data are the chaotic time series, generated by some linear dynamical systems.\n",
    "\n",
    "Paper skupia się tylko na czterech datasetach: logistic, Mackey-Glass, fractal Weierstrass, BET index. Nas chyba nie obchodzą sztucznie generowane datasety?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Long memory process is a process with a random component, where a past event has a decreasing/decaying effect on future events.\n",
    "\n",
    "Definicja, być może niepotrzebna."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "“Nonlinear Autoregressive models with eXogenous input (NARX model)”, which are therefore called NARX recurrent neural networks [1][4][5]. This is a powerful class of models which has been demonstrated that they are well suited for modeling nonlinear systems and specially time series.\n",
    "\n",
    "One principal application of NARX dynamic neural networks is in control systems.\n",
    "\n",
    "Also, is a class computationally equivalent to Turing Machines.\n",
    "\n",
    "Some important qualities about NARX networks with gradient-descending learning gradient algorithm have been reported: (1) learning is more effective in NARX networks than in other neural network (the gradient descent is better in NARX) and (2) these networks converge much faster and generalize better than other networks [4][5].\n",
    "\n",
    "paper jest z 2008 roku, człowiek ani razu nie używa nazwy \"LSTM\", co jest mocno podejrzane...\n",
    "\n",
    "NARX =?= NARMAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"narx.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Nieco bardziej precyzyjną definicję NARXów możemy znaleźć w Simon Haykin, \"Neural Networks and Learning Machines\".\n",
    "\n",
    "NARX po ludzku: zwykła sieć rekurencyjna w k-tym kroku bierze input u_k oraz output z poprzedniego kroku, który interpretujemy jako predykcję u_k i oznaczamy y_k, następnie wielkości te są konkatenowane i przepuszczane przez funkcję f (np. multilayer perceptron); NARX różni się tylko tym, że w k-tym kroku bierze (u_k, u_k-1, ..., u_k-q+1, y_k, y_k-1, ..., y_k-q+1), czyli q ostatnich inputów i outputów; definicja jest na tyle szeroka, że pozwala uwzględniać wielowymiarowe inputy, outputy, praktycznie dowolną postać f itp."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Uczenie:\n",
    "- dynamic backpropagation =?= backprop. through time\n",
    "- L_2 regularization\n",
    "chyba nic niezwykłego/odkrywczego."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "An interesting comparison about length of input lags in feedforward neural network and the NARX model presented in this paper can be made with [3]. It is well known from practice of NN that the input variables of NN should not to be much correlated, because the correlated input variables may degrade the forecasting by interacting with each other as well as other elements and producing a biased effect [23].\n",
    "\n",
    "Ważna uwaga (jeśli prawdziwa)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The first conclusion of this paper is that NARX recurrent neural networks have the potential to capture the dynamics of nonlinear dynamic system such as in the examples shown, for the Mackey-Glass system with different delays.\n",
    "VS ?\n",
    "The second conclusion is that the nonlinear NARX models are not without problems, they have limitation in learning long time dependences due to the “vanishing gradient”, and like any dynamical system are affected by instability, and have lack of a procedure of optimizing embedded memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
