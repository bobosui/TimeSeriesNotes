{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "28b4765d-8aa2-4d9d-ae5b-29028df32335"
    }
   },
   "source": [
    "# Regularization of LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "85b77aca-7694-4eb8-aa90-3b9d5c5759fe"
    }
   },
   "source": [
    "Well known regularization methods:  \n",
    "* [DropOut](https://i.stack.imgur.com/CewjH.png)\n",
    "* [DropConnect](https://i.stack.imgur.com/D1QC7.png)\n",
    "* early stopping\n",
    "* $L_2$ and $L_1$ regularization\n",
    "* weight noise\n",
    "* (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "41729cf1-40e9-4514-af97-9d045cab3fff"
    }
   },
   "source": [
    "## Stabilizing Activations (Krueger & Memisevic, 26 Apr 2016)\n",
    "\n",
    "Dodatkowy wyraz do funkcji kosztu:\n",
    "$$\n",
    "\\beta \\frac{1}{T} \\sum_{t=1}^{T}(||h_t||_2 -||h_{t-1}||_2)^2\n",
    "$$\n",
    "\n",
    "> Unlike the “temporal coherence” penalty of Jonschkowski & Brock (2015), our penalty does not encourage the state representation to remain constant, only its norm.\n",
    "\n",
    "\n",
    "Stabilność jest szczególnie ważna, jeśli podczas testowania chcemy generalizować dla dłuższych sekwencji wejściowych niż podczas uczenia (ogólnie dla RNN).\n",
    "\n",
    "Autorzy zauważyli, że LSTMy zachowują normę pamięci dłużej niż horyzont czasowy podczas trenowania, i to może być właśnie tym, co powodują ich użyteczność przy przewidywaniu długich sekwencji.\n",
    "\n",
    "[IRNN = Identity Recurrent Neural Networks](https://gist.github.com/GabrielPereyra/353499f2e6e407883b32)\n",
    "a nie Image Recognition...\n",
    "\n",
    "Podobne techniki (dla LSTMów):  \n",
    "- clipping memory\n",
    "- clippedReLU,  \n",
    "ale działają elementwise, a oni chcieli na całośći.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b690a440-8abf-41df-91d9-c1391bbd1579"
    }
   },
   "source": [
    "#### Eksperymenty\n",
    "\n",
    "Stabilizacja normy stanu ukrytego lub pamięci.\n",
    "\n",
    "Table 1 - ciekawa rozbieżność dla $\\beta=0$, co dla większych $\\beta$, jaki błąd?\n",
    "\n",
    "Figure 3 - [LSTMs] _exhibit **natural** stability_ - właściwość LSTMów\n",
    "\n",
    "Figure 6 - jak stabilizacja wpływa na bramki zapominania\n",
    "\n",
    "#### Podsumowanie\n",
    "\n",
    "> The best performance is achieved by penalizing the squared difference of subsequent hidden states’ norms. This penalty, the _norm-stabilizer_, improved performance on the tasks of language modeling and addition tasks, and gave state of the art RNN performance on phoneme recognition on the TIMIT dataset.\n",
    "\n",
    "A po mojemu:\n",
    "\n",
    "RNN się robią jak LSTMy, a LSTM jeszcze lepsze.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1ff5f9b6-aefc-4b9c-b202-2b4d22d776c4"
    }
   },
   "source": [
    "## Zoneout (Krueger et al., 13 Jun 2016)\n",
    "\n",
    "Głowna idea: zamiast ustawiać aktywacje neuronów na zero (jak w dropałcie), używamy aktywacji z poprzednioego kroku czasowego (czyli neurony się tak jakby zawieszają - to moja intuicja, inni mówią na to freeze).\n",
    "\n",
    "#### Wzory dla RNN:\n",
    "\n",
    "Zoneout $\\mathcal{T}_t = d_t \\odot \\tilde{\\mathcal{T}} + (1-d_t) \\odot \\mathbb{1}$\n",
    "\n",
    "Dropout $\\mathcal{T}_t = d_t \\odot \\tilde{\\mathcal{T}} + (1-d_t) \\odot \\mathbb{0}$\n",
    "\n",
    "gdzie $d_t$ - losowy wektor Bernoulliego.\n",
    "\n",
    "#### Wzory dla LSTM:\n",
    "\n",
    "Dropout i zoneout - w artykule rysunki i wzory.\n",
    "\n",
    "\n",
    "#### Wyniki\n",
    "\n",
    "pMNIST - gorsze niż Batch Normalization - Table 1.\n",
    "\n",
    "Penn Treebank Language Modeling Dataset - state of the art  - Table 2, Figure 4.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "75237565-5766-4145-8ebd-43b30deddae5"
    }
   },
   "source": [
    "## Surprisal Zoneout (Rocki et al., 13 Dec 2016)\n",
    "\n",
    "#### Motywacja: \n",
    "\n",
    ">According to Redundancy-Reducing Hypothesis (Barlow, 1961) neurons within the brain can code messages using different number of impulses. This indicates that the most probable events should be assigned codes with fewer impulses in order to minimize energy expenditure, or, in other words, that the more frequently occuring patterns in lower level neurons should trigger sparser activations in higher level ones. Keeping that in mind, we have focused on the problem of adaptive regularization, i.e. minimization of a number of neurons being activated depending on the novelty of the current input.\n",
    "\n",
    "#### Dalsza motywacja:\n",
    "\n",
    ">As learning progresses, the activations of that cell become less frequent in time and more iterations will just skip memorization, thus the proposed mechanism in fact enables different memory cells to operate on different time scales.\n",
    "\n",
    "#### Koncepcja:\n",
    "Każdy neuron niezależnie znajduje swój zoneout.\n",
    "\n",
    "#### Wyniki:\n",
    "State of the art on enwik8 and linux kernel.\n",
    "\n",
    "Rysunki z mapami(t)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ac3a5899-29af-4344-8ba9-98a2771fc1d8"
    }
   },
   "source": [
    "### Źródła:\n",
    "1. [Regularizing RNNs by Stabilizing Activations](https://arxiv.org/abs/1511.08400)\n",
    "2. [Zoneout](https://arxiv.org/abs/1606.01305)\n",
    "3. [Surprisal-Driven Zoneout](https://arxiv.org/abs/1610.07675)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
