{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting engine reliability by support vector machines, Wei-Chiang Hong, Ping-Feng Pai (2006)\n",
    "\n",
    "### Cel pracy:\n",
    "zbadanie czy maszyny wektorów nośnych nadają się do przewidywania niezawodności silników (enigine reliability)\n",
    "\n",
    "### Wnioski:\n",
    "Tak, nadają się.\n",
    "\n",
    "### Jakieś ważne założenia:\n",
    "* engine reliability changes with time\n",
    "* SVMs have been succesfully applied in solving **nonlinear regression and time series problems**\n",
    "\n",
    "\n",
    "### State of the art:\n",
    "* ARIMA dla szeregów czasowych w ogóle oraz specyficznie dla problemu ER\n",
    "* RNNy też niezłe dla ER\n",
    "* obiecujące eksperymenty na różnych sieciach neuronowych (uwaga: rok 2006)\n",
    "* SVMS\n",
    "    * SVMs dla cen na giełdzie, szeregów czasowych w zastosowaniach finansowych, przewidywania jakości powietrza czy prędkości wiatru\n",
    "    * DSVMs (D stands for dynamic -> [12]) dla niestacjonarych szeregów czasowych\n",
    "    * C-ascending SVMs dla niestacjonarnych szeregów czasowych w zastosowaniach finansowych\n",
    "\n",
    "### Modele wybrane przez autorów do porównań:\n",
    "Autorzy pragną sprawdzić, czy SVMy nadają się do przewidywania niezawodności silników. W tym celu porównają SVMy do:\n",
    "* Duane\n",
    "* ARIMA\n",
    "* GRNN\n",
    "\n",
    "#### Duane\n",
    "http://reliawiki.org/index.php/Duane_Model\n",
    "\n",
    "#### ARIMA\n",
    "Autoregresive Integrated Moving Average.\n",
    "\n",
    "\n",
    "#### GRNN\n",
    "General Regression Neural Network składa się z czterech wartsw:\n",
    "1. input\n",
    "2. *pattern neurons* odpowiedzialne za zapamiętywanie relacji między wejściem a pożądanym wyjściem\n",
    "3. *summation neurons*:\n",
    "    * $S_s = \\sum\\limits_i \\theta_i$\n",
    "    * $S_w = \\sum\\limits_i w_i\\theta_i$,\n",
    "    \n",
    "    gdzie $w_i$ waga i-tego *pattern* neuronu, $\\theta_i$ wyjście i-itego *pattern* neuronu\n",
    "4. wyjście wyliczane zgodnie ze wzorem: $Q =\\frac{S_w}{S_s}$\n",
    "\n",
    "#### SVM\n",
    "jak działają SVMy zaproponowane przez autorów?\n",
    "\n",
    "### Dobór hiperparametrów:\n",
    "#### Duane\n",
    "\n",
    "<img src='../img/er_svms/duane.png'>\n",
    "\n",
    "#### ARIMA\n",
    "W celu zredukowania ciągą niestacjonarnego do stacjonarnego, wartość parametru $d$ została wybrana na $1$.\n",
    "\n",
    "Pozostałe dwa parametry: $p = 4$ oraz $q = 3$ zostały wybrane przy użyciu funkcji autokorelacji i funkcji częściowej autokorelacji - vide prezentacja Igora Sieradzkiego. \n",
    "\n",
    "Ostatecznie równanie ma postać:\n",
    "\n",
    "$(1+0.74 B^1 -0.76 B^2 -0.93 B^3 -0.11 B^4) \\nabla^1 y_t + 0.0012$ \n",
    "$= (1 + 0.95 B^1-0.27b^2-0.72B^3)\\epsilon_t$\n",
    "\n",
    "\n",
    "#### GRNN\n",
    "Potrzeba jedynie znaleźć parametr wygładzania (*smoothing parameter*) $\\sigma$. Sprawdzane jest $n$ wybranych wartości.\n",
    "\n",
    "<img src='../img/er_svms/grnn_hyps.png' width=500px>\n",
    "\n",
    "#### SVM\n",
    "<img src='../img/er_svms/hyperparameters_SVM.png' align='right' style=\"PADDING-RIGHT: 10px\" width=280px>\n",
    "\n",
    "1. Wybierz przykładową wartość $\\epsilon$ oraz przykładową wartość $C$. Wybierz zbiór wartości parametru $\\sigma$. Dla wszystkich zestawów naucz SVMa. Zapamiętaj wartość $\\sigma'$, dla której model miał najmniejszy błąd NRMSE na zbiorze walidacyjnym.\n",
    "2. Wybierz przykładową wartość $\\epsilon$. Wybierz zbiór wartości dla parametru $C$. $\\sigma$ będzie równe $\\sigma'$ znalezionemu w poprzednim kroku. Dla wszystkich zestawów naucz SVMa. Zapamiętaj wartość $C'$, dla której model miał najmniejszy błąd NRMSE na zbiorze walidacyjnym.\n",
    "3. Wybierz zbiór wartości dla hiperparametru $\\epsilon$. $\\sigma$ będzie równe $\\sigma'$ znalezionemu w poprzednim kroku. $C$ będzie równe $C'$ znalezionemu w poprzednim kroku. Dla wszystkich zestawów naucz SVMa. Zapamiętaj wartość $\\epsilon'$, dla której model miał najmniejszy błąd NRMSE na zbiorze walidacyjnym.\n",
    "\n",
    "[obrazek na tablicy w 2D]\n",
    "\n",
    "Miara: **NRMSE** (*normalised root of mean squared errors*) \n",
    "$$NRMSE = \\sqrt{\\frac{\\sum^n_{i=1}(a_i-f_i)^2}{\\sum^n_{i=1} a_i^2}}$$.\n",
    "\n",
    "<img src='../img/er_svms/hiperparametry.png' vspace=30px width=500px>\n",
    "\n",
    "Zaczęli od $\\sigma = 0.5$ oraz $C = 50$. Czemu nie iterowali?\n",
    "\n",
    "\n",
    "### Dane:\n",
    "\n",
    "**U.S.S. Grampus Data** a może Halfbeak?\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/b/b1/Grampus_%28SS-523%29.jpg' width=200px align='left' style=\"PADDING-RIGHT: 10px\">\n",
    "\n",
    "<img src='../img/er_svms/grampus_data.png' width=300px align='right' style=\"PADDING-LEFT: 10px\">\n",
    "\n",
    "Często wykorzystywany zbiór danych w problemie ER. Grampusy pływały na silnikach Diesla. \n",
    "\n",
    "\"*Data-set is the data of arrival times to unscheduled maintenance actions  for  the  USS  Grampus  no.  4  main  propulsion  diesel engine*\".\n",
    "\n",
    "Wartością, którą model ma przewidzieć, jest czas następnej awarii silnika.\n",
    "\n",
    "<img src='../img/er_svms/pub_data.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wyniki eksperymentalne:\n",
    "\n",
    "<img src='../img/er_svms/eksperymenty.png'>\n",
    "\n",
    "\n",
    "**NRMSE** (normalised root mean-square deviation) $NRMSE = \\frac{RMSE}{y_{max}-y_{min}}$.\n",
    "\n",
    "### Wnioski\n",
    "SVM użyty przez autorów osiągnął zadowalające wyniki i wydaje się być alternatywą dla dotychczas używanych modeli w problemi ER.\n",
    "\n",
    "### Dyskusja\n",
    "* dałoby się znaleźć lepsze hiperparametry dla SVMa\n",
    "* zbiór wydaje się dosyć skąpy (mniej niż 100 przykładów uczących), co stawia pod pytajnikiem także wyniki eksperymentalne - czy sprawdzenie 14 przykładów daje wynik, który możemy uznać za wiarygodny?\n",
    "* zresztą czy różnica rzędzy 0.3 NRMSE może zostać użyta za znaczącą?\n",
    "* autorzy nie piszą o jakiejkolwiek normalizacji danych wejściowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 20.8906465 ,  18.62266375,  17.92552777,  17.84711084,\n",
       "        17.84366257,  17.84360186,  17.84360143,  17.84360143,\n",
       "        17.84360143,  17.84360143,  17.84360143,  17.84360143,\n",
       "        17.84360143,  17.84360143])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "import numpy as np\n",
    "\n",
    "X = np.arange(57).reshape(-1,1)\n",
    "y = np.loadtxt('data.csv')[:57]\n",
    "\n",
    "X2 = np.arange(57,71).reshape(-1,1)\n",
    "\n",
    "clf = SVR(C=60.0, epsilon=0.27, gamma=0.45,kernel='rbf')\n",
    "clf.fit(X,y)\n",
    "clf.predict(X2) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
