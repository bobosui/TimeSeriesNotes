{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jak nauczyć się sieci rekurencyjnych\n",
    "- przeczytać 3 razy http://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html\n",
    "- kolorowe obrazki są tu http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restricted Isometry Property (compressed sensing)\n",
    "\n",
    "https://en.wikipedia.org/wiki/Restricted_isometry_property\n",
    "http://www.math.ucla.edu/~tao/preprints/sparse.html\n",
    "https://class.ee.washington.edu/546/2010spr/papers/Baraniuk-RIP-JL.pdf\n",
    "https://en.wikipedia.org/wiki/Compressed_sensing\n",
    "http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\n",
    "\n",
    "Losowa macierz jest prawie izometrią:\n",
    "- bierzemy macierz n x m,\n",
    "- każdy element losujemy z rozkładu o średniej zero i wariancji 1/n,\n",
    "- rozkład może być dowolny - Glorot bierze rozkład jednostajny na przedziale, któryś z powyższych linków sugeruje, że rozkłady Gaussa dają najlepsze przybliżenie izometrii,\n",
    "- każda kolumna ma długość średnio jeden, iloczyn skalarny dwóch kolumn jest średnio zero,\n",
    "- oczywiście im m większe od n, tym gorzej jest upakować w R^n m wektorów wzajemnie prawie prostopadłych, ale podobno to dość dobrze działa,\n",
    "- compressed sensing - podobno jeśli dane są sparse (ale nie wiemy, wzdłuż jakich kierunków), to przepuszczenie ich przez losową projekcję traci bardzo mało informacji\n",
    "- dygresja - Glorot chce, aby gradienty przy backpropagation też były potraktowane izometrią, dlatego chciałby też, żeby wiersze były prawie prostopadłe (dlaczego? zrozumieć wzory); w związku z tym chciałby jednocześnie wariancję 1/n i 1/m, stąd n+m w mianowniku jako, wg niego, \"kompromis\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jak uczyć sieć rekurencyjną\n",
    "\n",
    "Backpropagation + np. adam, ale...\n",
    "\n",
    "\n",
    "### Sieć rekurencyjna vs zwykła\n",
    "http://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html#backpropagation-through-time-and-vanishing-sensitivity\n",
    "szczególnie warto zrozumieć ten kawałek wpisu na blogu\n",
    "\n",
    "### Truncated Backpropagation\n",
    "http://r2rt.com/styles-of-truncated-backpropagation.html\n",
    "\n",
    "\"Suppose we are training an RNN on sequences of length 10,000. If we apply non-truncated backpropagation through time, the entire sequence is fed into the network at once, the error at time step 10,000 will be back propagated all the way back to time step 1. The two problems with this are that it is (1) expensive to backpropagate the error so many steps, and (2) due to vanishing gradients, backpropagated errors get smaller and smaller layer by layer, which makes further backpropagation insignificant.\"\n",
    "\n",
    "\"Truncated backpropagation processes the sequence one timestep at a time, and every k1 timesteps, it runs BPTT for k2 timesteps.\"\n",
    "\n",
    "Dwa rodzaje truncated backpropagation:\n",
    "- k1 == k2\n",
    "- k1 == 1\n",
    "\n",
    "### Real-Time Recurrent Learning\n",
    "- propagowanie gradientu w przyszłość?\n",
    "- algorytm dobry do uczenia online?\n",
    "- jak to działa? czytamy:\n",
    "    - http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.52.9724&rep=rep1&type=pdf\n",
    "    - https://www.willamette.edu/~gorr/classes/cs449/rtrl.html\n",
    "    - http://www.dlsi.ua.es/~mlf/nnafmc/pbook/node29.html\n",
    "    - https://web.stanford.edu/class/psych209a/ReadingsByDate/02_25/Williams%20Zipser95RecNets.pdf (s. 453)\n",
    "    - http://www.sciencedirect.com/science/article/pii/S0925231298000897\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful (w sensie kerasa) LSTM\n",
    "\n",
    "http://philipperemy.github.io/keras-stateful-lstm/\n",
    "https://keras.io/layers/recurrent/\n",
    "https://github.com/fchollet/keras/issues/98\n",
    "\n",
    "- LSTM ma \"pamięć\" (c, h)\n",
    "- jak inicjalizujemy pamięć w pierwszym kroku czasowym? (zerami, losowo itp.)\n",
    "- stateful - pamięć na końcu i-tego przykładu w batchu jest pamięcią na początku i-tego przykładu w następnym batchu\n",
    "\n",
    "po co?:\n",
    "- np. chcemy pociąć długie szeregi na batche i umożliwić kerasowi zapamiętanie stanu\n",
    "- coś jeszcze?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10k różnych LSTMów\n",
    "\n",
    "http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf\n",
    "\n",
    "- główne pytanie - dlaczego LSTM/GRU tak, a nie inaczej?\n",
    "- jakimś algorytmem genetycznym wygenerowali 10000 architektur LSTM (mutacje w grafie obliczeń)\n",
    "- po wstępnym odfiltrowaniu zostawili 1000\n",
    "- każdą przetestowali na 230 zestawach hiperparametrów\n",
    "- wnioski:\n",
    "    - jak wyszło coś działającego, to raczej było podobne do LSTM/GRU, więc nic bardzo odmiennego od wymyślonych \"intuicyjnie\" architektur nie znaleźli\n",
    "    - jak się doda bias == 1 do forget gate w LSTM, to osiąga skuteczność GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co rysować podczas trenowania\n",
    "\n",
    "cs231n.github.io/neural-networks-3\n",
    "\n",
    "### TensorBoard\n",
    "Jeśli ktoś klepie w TensorFlow (ktoś klepie? pewnie nikt), to może użyć TensorBoard:\n",
    "https://www.tensorflow.org/versions/r0.11/tutorials/monitors/\n",
    "\n",
    "### Keras\n",
    "Przede wszystkim callbacks + własne metryki (keras reportuje nie tylko objective, ale też wybrane metryki)\n",
    "\n",
    "### \"Oczywiste\"\n",
    "- train/valid/test loss/inne_metryki\n",
    "- może zmiany learning rate?\n",
    "\n",
    "### Wartości na warstwach\n",
    "http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\n",
    "Glorot bada saturację warstw aktywacji, obserwując ich wartości:\n",
    "- mean/std outputów z warstw aktywacji - np. jeśli średnia po sigmoidzie jest w okolicy 0 lub 1, to źle,\n",
    "- \"Activation values normalized histogram at the end of learning, averaged across units of the same layer and across 300 test examples\",\n",
    "- może warto połączyć dwa powyższe w wykres 3D? czas (numer epoki) x histogramy?\n",
    "\n",
    "### Gradient\n",
    "Interesują nas gradienty dla każdego parametru w każdej warstwie. W wypadku sieci rekurencyjnych chcemy też wiedzieć, jak gradient propaguje się wstecz w czasie.\n",
    "https://github.com/fchollet/keras/issues/2226\n",
    "Jak to zrobić? W kerasie mogą być problemy, w tensorflow podobno jest łatwiej - TODO dowiedzieć się.\n",
    "\n",
    "Obrazek z drugiego paperu o Batch Normalization w sieciach rekurencyjnych:\n",
    "<img src='img/sieci_rekurencyjne/gradient_backprop_through_time.png' width = 800px>\n",
    "\n",
    "http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\n",
    "\n",
    "Glorot robi podobne rzeczy, co powyżej dla wartości. W szczególności konkluduje, że bez jego inicjalizacji gradienty na niższych warstwach mają dużą wariancję; z jego inicjalizacją wariancje są konsystentne pomiędzy warstwami.\n",
    "\n",
    "#### Gradient checking\n",
    "https://www.tensorflow.org/api_docs/python/test/gradient_checking\n",
    "Można sobie też od czasu do czasu wyestymować gradienty z wzoru f(x + delta) - f(x - delta) / 2 delta\n",
    "\n",
    "### Delta wartości wag\n",
    "Na cs231n piszą, żeby patrzeć na stosunek zmiany wag do ich wartości - wg nich powinno być ok. 0,1%.\n",
    "\n",
    "### Wizualizacja warstw\n",
    "Oczywiście jeśli się da - rzeczy typu \"pierwsza warstwa cnn nauczyła się krawędzi [tu obrazki krawędzi], siódma nauczyła się kotów [obrazek kota]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fajne słowa\n",
    "\n",
    "- Neural Turing Machines - https://arxiv.org/abs/1410.5401\n",
    "- Attention LSTM/coś -http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
